{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Resnet Model Features\n",
    "\n",
    "We know that the Resnet model works well, but why does it work? How can we have confidence that it is searching out the correct features? A recent paper, [Axiomatic Attribution for Deep Networks](https://arxiv.org/pdf/1703.01365.pdf), shows that averaging gradients taken along a path of images from a blank image (e.g. pure black or grey) to the actual image, can robustly predict sets of pixels that have a strong impact on the overall classification of the image. The below code shows how to modify the TF estimator code to analyze model behavior of different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_IMAGE_SIZE = 224\n",
    "_NUM_CHANNELS = 3\n",
    "_LABEL_CLASSES = 1001\n",
    "\n",
    "RESNET_SIZE = 50  # We're loading a resnet-50 saved model.\n",
    "\n",
    "# Model directory\n",
    "MODEL_DIR='resnet_model_checkpoints'\n",
    "VIS_DIR='visualization'\n",
    "\n",
    "# RIEMANN STEPS is the number of steps in a Riemann Sum.\n",
    "# This is used to compute an approximate the integral of gradients by supplying\n",
    "# images on the path from a blank image to the original image.\n",
    "RIEMANN_STEPS = 30\n",
    "\n",
    "# Return the top k classes and probabilities, so we can also visualize model inference\n",
    "# against other contending classes besides the most likely class.\n",
    "TOP_K = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download model checkpoint\n",
    "\n",
    "The next step is to load the researcher's saved checkpoint into our estimator. We will download it from\n",
    "http://download.tensorflow.org/models/official/resnet50_2017_11_30.tar.gz using the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"http://download.tensorflow.org/models/official/resnet50_2017_11_30.tar.gz \", \"resnet.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip the file into a directory called resnet\n",
    "call([\"mkdir\", MODEL_DIR])\n",
    "call([\"tar\", \"-zxvf\", \"resnet.tar.gz\", \"-C\", MODEL_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you see model checkpoint files in this directory\n",
    "os.listdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Model Architecture\n",
    " \n",
    "In order to reconstruct the Resnet neural network used to train the Imagenet model, we need to load the architecture pieces. During the setup step, we checked out https://github.com/tensorflow/models/tree/v1.4.0/official/resnet. We can now load functions and constants from resnet_model.py into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../models/official/resnet/resnet_model.py  #TODO: modify directory based on where you git cloned the TF models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image preprocessing functions\n",
    "\n",
    "Note that preprocessing functions are called during training as well (see https://github.com/tensorflow/models/blob/master/official/resnet/imagenet_main.py and https://github.com/tensorflow/models/blob/master/official/resnet/vgg_preprocessing.py), so we will need to extract relevant logic from these functions. Below is a simplified preprocessing code that normalizes the image's pixel values.\n",
    "\n",
    "For simplicity, we assume the client provides properly-sized images 224 x 224 x 3 in batches. It will become clear later that sending images over ip in protobuf format can be more easily handled by storing a 4d tensor. The only preprocessing required here is to subtract the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "  \"\"\"Preprocesses the image by subtracting out the mean from all channels.\n",
    "  Args:\n",
    "    image: A 4D `Tensor` representing a batch of images.\n",
    "  Returns:\n",
    "    image pixels normalized to be between -0.5 and 0.5\n",
    "  \"\"\"\n",
    "  return tf.to_float(images) / 255 - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet Model Functions\n",
    "\n",
    "We are going to create two estimators here since we need to run two model predictions. \n",
    "\n",
    "* The first prediction computes the top labels for the image by returning the argmax_k top logits. \n",
    "\n",
    "* The second prediction returns a sequence of gradients along the straightline path from a purely grey image (127.5, 127.5, 127.5) to the final image. We use grey here because the resnet model transforms this pixel value to all 0s.\n",
    "\n",
    "Below is the resnet model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_model_fn(features, labels, mode):\n",
    "  \"\"\"Our model_fn for ResNet to be used with our Estimator.\"\"\"\n",
    "\n",
    "  # Preprocess images as necessary for resnet\n",
    "  features = preprocess_images(features['images'])\n",
    "\n",
    "  # This network must be IDENTICAL to that used to train.\n",
    "  network = imagenet_resnet_v2(RESNET_SIZE, _LABEL_CLASSES)\n",
    "\n",
    "  # tf.estimator.ModeKeys.TRAIN will be false since we are predicting.\n",
    "  logits = network(\n",
    "      inputs=features, is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "  # Instead of the top 1 result, we can now return top k!\n",
    "  top_k_logits, top_k_classes = tf.nn.top_k(logits, k=TOP_K)\n",
    "  top_k_probs = tf.nn.softmax(top_k_logits)\n",
    "  predictions = {\n",
    "      'classes': top_k_classes,\n",
    "      'probabilities': top_k_probs\n",
    "  }\n",
    "\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=predictions, \n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients Model Function\n",
    "\n",
    "The Gradients model function takes as input a single image (a 4d tensor of dimension [1, 244, 244, 3]) and expands it to a series of images (tensor dimension [RIEMANN_STEPS + 1, 244, 244, 3]), where each image is simply a \"fractional\" image, with image 0 being pure gray to image RIEMANN_STEPS being the original image. The gradients are then computed for each of these images, and various outputs are returned.\n",
    "\n",
    "**Note:** Each step is a single inference that returns an entire gradient pixel map.\n",
    "The total gradient map evaluation can take a couple minutes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients_model_fn(features, labels, mode):\n",
    "  \"\"\"Our model_fn for ResNet to be used with our Estimator.\"\"\"\n",
    "    \n",
    "  # Supply the most likely class from features dict to determine which logit function\n",
    "  # to use gradients along the\n",
    "  most_likely_class = features['most_likely_class']\n",
    "    \n",
    "  # Features here is a 4d tensor of ONE image. Normalize it as in training and serving.\n",
    "  features = preprocess_images(features['images'])\n",
    "\n",
    "  # This network must be IDENTICAL to that used to train.\n",
    "  network = imagenet_resnet_v2(RESNET_SIZE, _LABEL_CLASSES)\n",
    "\n",
    "  # path_features should have dim [RIEMANN_STEPS + 1, 224, 224, 3]\n",
    "  path_features = tf.zeros([1, 224, 224, 3])\n",
    "  for i in range(1, RIEMANN_STEPS + 1):\n",
    "    path_features = tf.concat([path_features, features * i / RIEMANN_STEPS], axis=0)\n",
    "   \n",
    "  # Path logits should evaluate logits for each path feature and return a 2d array for all path images and classes\n",
    "  path_logits = network(inputs=path_features, is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "  # The logit we care about is only that pertaining to the most likely class\n",
    "  # The most likely class contains only a single integer, so retrieve it.\n",
    "  target_logits = path_logits[:, most_likely_class[0]]\n",
    "   \n",
    "  # Compute gradients for each image with respect to each logit\n",
    "  gradients = tf.gradients(target_logits, path_features)\n",
    "    \n",
    "  # Multiply elementwise to the original image to get weighted gradients for each pixel.\n",
    "  gradients = tf.squeeze(tf.multiply(gradients, features))\n",
    "    \n",
    "  predictions = {\n",
    "      'path_features': path_features,  # for debugging\n",
    "      'path_logits': path_logits,  # for debugging\n",
    "      'target_logits': target_logits,  # use this to verify that the riemann integral works out\n",
    "      'path_features': path_features, # for displaying path images\n",
    "      'gradients': gradients  # for displaying gradient images and computing integrated gradient\n",
    "  }\n",
    "\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "    mode=mode,\n",
    "    predictions=predictions,  # This is the returned value\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators\n",
    "\n",
    "Load in the model_fn using the checkpoints from MODEL_DIR. This will initialize our weights which we will then use to run backpropagation to find integrated gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load this model into our estimator\n",
    "resnet_estimator = tf.estimator.Estimator(\n",
    "  model_fn=resnet_model_fn,  # Call our generate_model_fn to create model function\n",
    "  model_dir=MODEL_DIR,  # Where to look for model checkpoints\n",
    "  #config not needed\n",
    ")\n",
    "\n",
    "gradients_estimator = tf.estimator.Estimator(\n",
    "  model_fn=gradients_model_fn,  # Call our generate_model_fn to create model function\n",
    "  model_dir=MODEL_DIR,  # Where to look for model checkpoints\n",
    "  #config not needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create properly sized image in numpy\n",
    "\n",
    "Load whatever image you would like (local or url), and resize to 224 x 224 x 3 using opencv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad_image(img, output_image_dim=_DEFAULT_IMAGE_SIZE):\n",
    "  \"\"\"Resize the image to make it IMAGE_DIM x IMAGE_DIM pixels in size.\n",
    "\n",
    "  If an image is not square, it will pad the top/bottom or left/right\n",
    "  with black pixels to ensure the image is square.\n",
    "\n",
    "  Args:\n",
    "    img: the input 3-color image\n",
    "    output_image_dim: resized and padded output length (and width)\n",
    "\n",
    "  Returns:\n",
    "    resized and padded image\n",
    "  \"\"\"\n",
    "\n",
    "  h, w = img.shape[:2]\n",
    "\n",
    "  # interpolation method\n",
    "  if h > output_image_dim or w > output_image_dim:\n",
    "    # use preferred interpolation method for shrinking image\n",
    "    interp = cv2.INTER_AREA\n",
    "  else:\n",
    "    # use preferred interpolation method for stretching image\n",
    "    interp = cv2.INTER_CUBIC\n",
    "\n",
    "  # aspect ratio of image\n",
    "  aspect = float(w) / h\n",
    "\n",
    "  # compute scaling and pad sizing\n",
    "  if aspect > 1:  # Image is \"wide\". Add black pixels on top and bottom.\n",
    "    new_w = output_image_dim\n",
    "    new_h = np.round(new_w / aspect)\n",
    "    pad_vert = (output_image_dim - new_h) / 2\n",
    "    pad_top, pad_bot = int(np.floor(pad_vert)), int(np.ceil(pad_vert))\n",
    "    pad_left, pad_right = 0, 0\n",
    "  elif aspect < 1:  # Image is \"tall\". Add black pixels on left and right.\n",
    "    new_h = output_image_dim\n",
    "    new_w = np.round(new_h * aspect)\n",
    "    pad_horz = (output_image_dim - new_w) / 2\n",
    "    pad_left, pad_right = int(np.floor(pad_horz)), int(np.ceil(pad_horz))\n",
    "    pad_top, pad_bot = 0, 0\n",
    "  else:  # square image\n",
    "    new_h = output_image_dim\n",
    "    new_w = output_image_dim\n",
    "    pad_left, pad_right, pad_top, pad_bot = 0, 0, 0, 0\n",
    "\n",
    "  # scale to IMAGE_DIM x IMAGE_DIM and pad with zeros (black pixels)\n",
    "  scaled_img = cv2.resize(img, (int(new_w), int(new_h)), interpolation=interp)\n",
    "  scaled_img = cv2.copyMakeBorder(scaled_img,\n",
    "                                  pad_top, pad_bot, pad_left, pad_right,\n",
    "                                  borderType=cv2.BORDER_CONSTANT, value=[127, 127, 127]) # Grey border\n",
    "  return scaled_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE = 'client/cat_sample.jpg'\n",
    "IMAGE_NAME = os.path.splitext(os.path.basename(IMAGE))[0]\n",
    "print(IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = None\n",
    "if 'http' in IMAGE:\n",
    "  resp = urllib.request.urlopen(IMAGE)\n",
    "  feature = np.asarray(bytearray(resp.read()), dtype='uint8')\n",
    "  feature = cv2.imdecode(feature, cv2.IMREAD_COLOR)\n",
    "else:\n",
    "  feature = cv2.imread(IMAGE)  # Parse the image from your local disk.\n",
    "\n",
    "feature = cv2.cvtColor(feature, cv2.COLOR_BGR2RGB)  # Flip the RGB (cv2 issue)\n",
    "# Resize and pad the image\n",
    "feature = resize_and_pad_image(feature)\n",
    "# Append to features_array\n",
    "feature = np.array([feature], dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image to validate\n",
    "imgplot = plt.imshow(feature[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Input Function\n",
    "\n",
    "Since we are analyzing the model using the estimator api, we need to provide an input function for prediction. Fortunately, there are built-in input functions that can read from numpy arrays, e.g. tf.estimator.inputs.numpy_input_fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predictions = resnet_estimator.predict(\n",
    "    tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'images': feature},\n",
    "        shuffle=False\n",
    "    )\n",
    ")\n",
    "\n",
    "label_dict = next(label_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out probabilities and class names\n",
    "classval = label_dict['classes']\n",
    "probsval = label_dict['probabilities']\n",
    "labels = []\n",
    "with open('client/imagenet1000_clsid_to_human.txt', 'r') as f:\n",
    "  label_reader = csv.reader(f, delimiter=':', quotechar='\\'')\n",
    "  for row in label_reader:\n",
    "    labels.append(row[1][:-1])\n",
    "# The served model uses 0 as the miscellaneous class, and so starts indexing\n",
    "# the imagenet images from 1. Subtract 1 to reference the text correctly.\n",
    "classval = [labels[x - 1] for x in classval]\n",
    "class_and_probs = [str(p) + ' : ' + c for c, p in zip(classval, probsval)]\n",
    "for j in range(0, 5):\n",
    "  print(class_and_probs[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Gradients\n",
    "\n",
    "Run the gradients estimator to retrieve a generator of metrics and gradient pictures, and pickle the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the visualization directory\n",
    "IMAGE_DIR = os.path.join(VIS_DIR, IMAGE_NAME)\n",
    "call(['mkdir', '-p', IMAGE_DIR])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one of the top classes. 0 picks out the best, 1 picks out second best, etc...\n",
    "best_label = label_dict['classes'][0]\n",
    "\n",
    "# Compute gradients with respect to this class\n",
    "gradient_predictions = gradients_estimator.predict(\n",
    "    tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'images': feature, 'most_likely_class': np.array([best_label])},\n",
    "        shuffle=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Start computing the sum of gradients (to be used for integrated gradients)\n",
    "int_gradients = np.zeros((224, 224, 3))\n",
    "gradients_and_logits = []\n",
    "\n",
    "# Print gradients along the path, and pickle them\n",
    "for i in range(0, RIEMANN_STEPS + 1):\n",
    "    gradient_dict = next(gradient_predictions)\n",
    "    gradient_map = gradient_dict['gradients']\n",
    "    print('Path image %d: gradient: %f, logit: %f' % (i, np.sum(gradient_map), gradient_dict['target_logits']))\n",
    "    # Gradient visualization output pickles\n",
    "    pickle.dump(gradient_map, open(os.path.join(IMAGE_DIR, 'path_gradient_' + str(i) + '.pkl'), \"wb\" ))\n",
    "    int_gradients = np.add(int_gradients, gradient_map)\n",
    "    gradients_and_logits.append((np.sum(gradient_map), gradient_dict['target_logits']))\n",
    "    \n",
    "pickle.dump(int_gradients, open(os.path.join(IMAGE_DIR, 'int_gradients.pkl'), \"wb\" ))\n",
    "pickle.dump(gradients_and_logits, open(os.path.join(IMAGE_DIR, 'gradients_and_logits.pkl'), \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "If you simply want to play around with visualization, unpickle the result from above so you do not have to rerun prediction again. The following visualizes the gradients with different amplification of pixels, and prints their derivatives and logits as well to view where the biggest differentiators lie. You can also modify the INTERPOLATION flag to increase the \"fatness\" of pixels.\n",
    "\n",
    "Below are two examples of visualization methods: one computing the gradient value normalized to between 0 and 1, and another visualizing absolute deviation from the median.\n",
    "\n",
    "## Plotting individual image gradients along path\n",
    "\n",
    "First, let us plot the individual gradient value for all gradient path images. Pay special attention to the images with a large positive gradient (i.e. in the direction of increasing logit for the most likely class). Do the pixel gradients resemble the image class you are trying to detect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMPLIFICATION = 2.0\n",
    "INTERPOLATION = 'none'\n",
    "\n",
    "gradients_and_logits = pickle.load(open(os.path.join(IMAGE_DIR, 'gradients_and_logits.pkl'), \"rb\" ))\n",
    "for i in range(0, RIEMANN_STEPS + 1):\n",
    "    gradient_map = pickle.load(open(os.path.join(IMAGE_DIR, 'path_gradient_' + str(i) + '.pkl'), \"rb\" ))\n",
    "    min_grad = np.ndarray.min(gradient_map)\n",
    "    max_grad = np.ndarray.max(gradient_map)\n",
    "    median_grad = np.median(gradient_map)\n",
    "    gradient_and_logit = gradients_and_logits[i]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(121)\n",
    "    plt.title('Image %d: grad: %.2f, logit: %.2f' % (i, gradient_and_logit[0], gradient_and_logit[1]))\n",
    "    imgplot = plt.imshow((gradient_map - min_grad) / (max_grad - min_grad),\n",
    "                        interpolation=INTERPOLATION)\n",
    "    plt.subplot(122)\n",
    "    plt.title('Image %d: grad: %.2f, logit: %.2f' % (i, gradient_and_logit[0], gradient_and_logit[1]))\n",
    "    imgplot = plt.imshow(np.abs(gradient_map - median_grad) * AMPLIFICATION / max(max_grad - median_grad, median_grad - min_grad),\n",
    "                       interpolation=INTERPOLATION)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Integrated Gradient\n",
    "\n",
    "When integrating over all gradients along the path, the result is an image that captures larger signals from pixels with the large gradients. Is the integrated gradient a clear representation of what it is trying to detect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMPLIFICATION = 2.0\n",
    "INTERPOLATION = 'none'\n",
    "\n",
    "# Plot the integrated gradients\n",
    "int_gradients = pickle.load(open(os.path.join(IMAGE_DIR, 'int_gradients.pkl'), \"rb\" ))\n",
    "min_grad = np.ndarray.min(int_gradients)\n",
    "max_grad = np.ndarray.max(int_gradients)\n",
    "median_grad = np.median(int_gradients)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(131)\n",
    "imgplot = plt.imshow((int_gradients - min_grad) / (max_grad - min_grad),\n",
    "                    interpolation=INTERPOLATION)\n",
    "plt.subplot(132)\n",
    "imgplot = plt.imshow(np.abs(int_gradients - median_grad) * AMPLIFICATION / max(max_grad - median_grad, median_grad - min_grad),\n",
    "                        interpolation=INTERPOLATION)\n",
    "plt.subplot(133)\n",
    "imgplot = plt.imshow(feature[0])\n",
    "plt.show()\n",
    "\n",
    "# Verify that the average of gradients is equal to the difference in logits\n",
    "print('total logit diff: %f' % (gradients_and_logits[RIEMANN_STEPS][1] - gradients_and_logits[0][1]))\n",
    "print('sum of integrated gradients: %f' % (np.sum(int_gradients) / RIEMANN_STEPS + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the integrated gradients for each channel\n",
    "\n",
    "We can also visualize individual pixel contributions from different RGB channels.\n",
    "\n",
    "Can you think of any other visualization ideas to try out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMPLIFICATION = 2.0\n",
    "INTERPOLATION = 'none'\n",
    "\n",
    "# Show red-green-blue channels for integrated gradients\n",
    "for channel in range(0, 3):\n",
    "    gradient_channel = int_gradients[:,:,channel]\n",
    "    min_grad = np.ndarray.min(gradient_channel)\n",
    "    max_grad = np.ndarray.max(gradient_channel)\n",
    "    median_grad = np.median(gradient_channel)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(121)\n",
    "    imgplot = plt.imshow((gradient_channel - min_grad) / (max_grad - min_grad),\n",
    "                         interpolation=INTERPOLATION,\n",
    "                         cmap='gray')\n",
    "    plt.subplot(122)\n",
    "    imgplot = plt.imshow(np.abs(gradient_channel - median_grad) * AMPLIFICATION / max(max_grad - median_grad, median_grad - min_grad),\n",
    "                         interpolation=INTERPOLATION,\n",
    "                         cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
