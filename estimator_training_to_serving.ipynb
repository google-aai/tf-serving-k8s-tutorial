{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving a TF Estimator Resnet Model\n",
    "\n",
    "**Scenario:** In TensorFlow 1.3, a higher level API called Estimators was introduced and has since been a popular API of choice within the TensorFlow community. Suppose that an ML researcher has trained a Resnet model on the Imagenet dataset using TensorFlow's Estimator API, located at https://github.com/tensorflow/models/tree/v1.4.0/official/resnet. (Note that we used v1.4.0. You always want to use a stable tag for a model version to deploy as the researcher can continue to modify the model and architecture at the head of master.) Our task is to deploy this model into TensorFlow Serving. You have access to their python code as well as a saved state (checkpoint) that points to their favorite trained result.\n",
    "\n",
    "This notebook teaches how to use the Estimator API to create a servable version of a pre-trained Resnet 50 model trained on ImageNet. The servable model can be served using [TensorFlow Serving](https://www.tensorflow.org/serving/), which runs very efficiently in C++ and supports multiple platforms (different OSes, as well as hardware with different types of accelerators such as GPUs). The model will need to handle RPC prediction calls coming from a client that sends requests containing a batch of jpeg images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble\n",
    "\n",
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a constant indicating the number of layers in our loaded model. We're loading a \n",
    "# resnet-50 model.\n",
    "RESNET_SIZE = 50  \n",
    "\n",
    "# Model and serving directories\n",
    "MODEL_DIR=\"resnet_model_checkpoints\"\n",
    "SERVING_DIR=\"estimator_servable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download model checkpoint\n",
    "\n",
    "Download the estimator saved checkpoint file from http://download.tensorflow.org/models/official/resnet50_2017_11_30.tar.gz, and extract to MODEL_DIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"http://download.tensorflow.org/models/official/resnet50_2017_11_30.tar.gz \", \"resnet.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip the file into a directory called resnet\n",
    "from subprocess import call\n",
    "call([\"mkdir\", MODEL_DIR])\n",
    "call([\"tar\", \"-zxvf\", \"resnet.tar.gz\", \"-C\", MODEL_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you see model checkpoint files in this directory\n",
    "os.listdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Model Architecture\n",
    " \n",
    "In order to reconstruct the Resnet neural network used to train the Imagenet model, we need to load the architecture pieces. During the setup step, we checked out https://github.com/tensorflow/models/tree/v1.4.0/official/resnet into the parent directory + \"/models\". We can now load functions and constants from resnet_model.py into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../models/official/resnet/resnet_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** We also need to import some constants from [imagenet_main.py](https://github.com/tensorflow/models/blob/v1.4.0/official/resnet/imagenet_main.py), but we cannot run this file as it is a main class that will attempt to train ResNet. Open [imagenet_main.py](https://github.com/tensorflow/models/blob/v1.4.0/official/resnet/imagenet_main.py) and copy over a few constants that are important--namely, the image size, channels, and number of classes--into the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy constants from imagenet_main.py.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Servable from the Estimator API\n",
    "\n",
    "The [TensorFlow Estimator API](https://www.tensorflow.org/programmers_guide/estimators) is an abstraction that simplifies the process of training, evaluation, prediction, and serving. Central to the Estimator API is an argument called the model function (model_fn). Essentially, a model function defines which graph nodes are used in training, evaluation, and prediction. Depending on the mode (TRAIN, EVAL, PREDICT) used, the model function will return an [EstimatorSpec](https://www.tensorflow.org/versions/r1.3/extend/estimators#constructing_the_model_fn) object tell the Estimator to run different graph nodes. The typical behavior of a model function would be:\n",
    "\n",
    "* TRAIN mode calls an optimizer that is hooked to a loss function (e.g. cross-entropy). This loss function is hooked to a node that contains the training labels, as well as a node that computes predicted logits for each class (which is hooked to nodes in lower layers of the network, etc., and finally hooked to the input placeholder nodes).\n",
    "* EVAL mode does not call the optimizer, but calls the loss function and potentially, other evaluation metric (e.g. accuracy). These evaluation metrics will likely depend on labels as well as the node computing predicted logits for each class.\n",
    "  * Additionally, researchers will often use monitors and hooks during training and evaluation to check on the progress of the model. Usually, these components are used to return summaries about different layers of the network, such as model coefficients, etc., which can be visualized using [Tensorboard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard).\n",
    "* PREDICT mode does NOT require an optimizer as there is no training step, and no label or loss functions (which depend on the label). Instead, predictions simply try to provide clients/users with information of interest, such as the most likely label for an image, the probability of the image being of a particular class, etc., which depend on graph components such as the logits node, etc.\n",
    "\n",
    "**Exercise:** Below is the training code used in the imagenet_main.py [resnet_model_fn()](https://github.com/tensorflow/models/blob/v1.4.0/official/resnet/imagenet_main.py#L162), renamed to serving_model_fn(). Portions of the code are modified and refactored into separate helper functions for debugging purposes. Since model serving is essentially prediction, graph elements associated with TRAIN and EVAL modes are no longer relevant. Remove/shortcut graph elements that are unrelated to prediction in the code cell below (marked with TODOs).\n",
    "\n",
    "**Useful References:**\n",
    "* [tf.estimator.EstimatorSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_model_fn(features, labels, mode):\n",
    "    '''The main model function used by the estimator to define the TensorFlow model server API.\n",
    "\n",
    "    Args:\n",
    "      features: The client request, which is a dictionary: {'image': 1D tensor of jpeg strings}\n",
    "      labels: None or not used since we are predicting only\n",
    "      mode: TRAIN, EVAL, or PREDICT. Serving only uses PREDICT mode.\n",
    "\n",
    "    Returns:\n",
    "      If training or evaluating (should not happen), return a blank EstimatorSpec that does\n",
    "        nothing.\n",
    "      If predicting (always), return an EstimatorSpec that produces a response with top k classes\n",
    "        and probabilities to send back to the client.\n",
    "    '''\n",
    "\n",
    "    # TODO: Remove tf.summary.image(). This is used for monitoring during training.\n",
    "    tf.summary.image('images', features, max_outputs=6)\n",
    "\n",
    "    # Move preprocessing, network, and postprocessing into a helper function.\n",
    "    # serving_input_to_output() will be defined below.\n",
    "    predictions = serving_input_to_output(features, mode)\n",
    "\n",
    "    # Create the PREDICT EstimatorSpec that will send a proper response back to the client.\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return create_servable_estimator_spec(predictions, mode)\n",
    "\n",
    "    # TODO: You already returned the EstimatorSpec for predictions.\n",
    "    # Training and evaluation are not needed.\n",
    "    # Shortcut every graph element below here by returning a minimal EstimatorSpec.\n",
    "    return ???\n",
    "    \n",
    "    # Calculate loss, which includes softmax cross entropy and L2 regularization.\n",
    "    cross_entropy = tf.losses.softmax_cross_entropy(\n",
    "        logits=logits, onehot_labels=labels)\n",
    "\n",
    "    # Create a tensor named cross_entropy for logging purposes.\n",
    "    tf.identity(cross_entropy, name='cross_entropy')\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "    # Add weight decay to the loss. We exclude the batch norm variables because\n",
    "    # doing so leads to a small improvement in accuracy.\n",
    "    loss = cross_entropy + _WEIGHT_DECAY * tf.add_n(\n",
    "        [tf.nn.l2_loss(v) for v in tf.trainable_variables()\n",
    "         if 'batch_normalization' not in v.name])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        # Scale the learning rate linearly with the batch size. When the batch size\n",
    "        # is 256, the learning rate should be 0.1.\n",
    "        initial_learning_rate = 0.1 * params['batch_size'] / 256\n",
    "        batches_per_epoch = _NUM_IMAGES['train'] / params['batch_size']\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "        # Multiply the learning rate by 0.1 at 30, 60, 80, and 90 epochs.\n",
    "        boundaries = [\n",
    "            int(batches_per_epoch * epoch) for epoch in [30, 60, 80, 90]]\n",
    "        values = [\n",
    "            initial_learning_rate * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\n",
    "        learning_rate = tf.train.piecewise_constant(\n",
    "            tf.cast(global_step, tf.int32), boundaries, values)\n",
    "\n",
    "        # Create a tensor named learning_rate for logging purposes.\n",
    "        tf.identity(learning_rate, name='learning_rate')\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "        optimizer = tf.train.MomentumOptimizer(\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=_MOMENTUM)\n",
    "\n",
    "        # Batch norm requires update_ops to be added as a train_op dependency.\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "          train_op = optimizer.minimize(loss, global_step)\n",
    "    else:\n",
    "        train_op = None\n",
    "\n",
    "    accuracy = tf.metrics.accuracy(\n",
    "        tf.argmax(labels, axis=1), predictions['classes'])\n",
    "    metrics = {'accuracy': accuracy}\n",
    "\n",
    "    # Create a tensor named train_accuracy for logging purposes.\n",
    "    tf.identity(accuracy[1], name='train_accuracy')\n",
    "    tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Building a TensorFlow Graph\n",
    "\n",
    "TensorFlow is essentially a computation graph with variables and states. The graph must be built before it can ingest and process data. Typically, a TensorFlow graph will contain a set of input nodes (called placeholders) from which data can be ingested, and a set of TensorFlow functions that take existing nodes as inputs and produces a dependent node that performs a computation on the input nodes.\n",
    "\n",
    "It is often useful to create helper functions for building a TensorFlow graphs for two reasons:\n",
    "\n",
    "1. Modularity: you can reuse functions in different places; for instance, a different image model or ResNet architecture can reuse functions.\n",
    "2. Testability: you can unit test different parts of your code easily!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function: convert JPEG strings to Normalized 3D Tensors\n",
    "\n",
    "In the API we are designing, [ResNet client](./client/resnet_client.py) sends a request which is an array (tensor) of JPEG-encoded images encoded as strings. For simplicity, these jpegs are all appropriately resized to 224x224x3 by the client, and do not need resizing on the server side to enter into the ResNet model. However, the ResNet50 model was trained with pixel values normalized (approximately) between -0.5 and 0.5. We will need to decode each JPEG string to extract the raw 3D tensor, and normalize the values.\n",
    "\n",
    "**Exercise:** Create a helper function that builds a TensorFlow graph component to decode a jpeg image, and normalizes pixel values to be between -0.5 and 0.5. (The normalization code is already done for you below.)\n",
    "\n",
    "**Useful References:**\n",
    "* [tf.image module](https://www.tensorflow.org/api_guides/python/image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_jpeg_to_image(encoded_image):\n",
    "  \"\"\"Preprocesses the image by subtracting out the mean from all channels.\n",
    "  Args:\n",
    "    image: A jpeg-formatted byte stream represented as a string.\n",
    "  Returns:\n",
    "    A 3d tensor of image pixels normalized to be between -0.5 and 0.5, resized to \n",
    "      height x width x 3.\n",
    "      The normalization approximates the preprocess_for_train and preprocess_for_eval functions\n",
    "      in https://github.com/tensorflow/models/blob/v1.4.0/official/resnet/vgg_preprocessing.py.\n",
    "  \"\"\"\n",
    "  image = ???  # TODO: Use a tf function to decode the jpeg into a 3d tensor.\n",
    "  image = tf.to_float(image) / 255.0 - 0.5  # Normalize values to be between -0.5 and 0.5.\n",
    "  return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit test the helper function\n",
    "\n",
    "**Exercise:** We are going to construct an input node in a graph, and use the helper function to add computational components on the input node. Afterwards, we will run the graph by providing sample input into the graph using a TensorFlow session.\n",
    "\n",
    "Input nodes for receiving data are called [placeholders](https://www.tensorflow.org/api_docs/python/tf/placeholder). A placeholder can store a Tensor of arbitrary dimension, and arbitrary length in any dimension. In the second step where the graph is run, the placeholder is populated with input data, and dependent nodes in your graph can then operate on the data and ultimately return an output. An example of a placeholder that holds a 1d tensor of floating values is:\n",
    "\n",
    "```\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[10], 'my_input_node')\n",
    "```\n",
    "\n",
    "Note that we assigned a Python variable x to be a pointer to the placeholder, but simply calling tf.placeholder() would create an element in the TensorFlow graph that can be referenced in a global dictionary as 'my_input_node'. However, it helps to keep a Python pointer to this graph element since we can more easily pass it into helper functions. \n",
    "\n",
    "Any dependent node in the graph can serve as an output node. For instance, passing an input node x through `y = build_jpeg_to_image_graph(x)` would return a node referenced by python variable y which is the result of processing the input through the operations in the helper function. When we run the test graph with real data below, you will see how to return the output of y.\n",
    "\n",
    "**Remember:** TensorFlow helper functions are used to help construct a computational graph! build_jpeg_to_image_graph() does not return a 3D array. It returns a graph node that returns a 3D array after processing a jpeg-encoded string!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining input test graph nodes: only needs to be run once!\n",
    "test_jpeg_ph = tf.placeholder(dtype=tf.string, shape=[], name='test_jpeg_placeholder')  # A placeholder for a single string, which is a dimensionless (0D) tensor.\n",
    "test_decoded_tensor = convert_jpeg_to_image(test_jpeg_ph)  # Output node, which returns a 3D tensor after processing.\n",
    "\n",
    "# Print the graph elements to check shapes.  ? indicates that TensorFlow does not know the length.\n",
    "# of those dimensions.\n",
    "print(test_jpeg_ph)\n",
    "print(test_decoded_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Test Graph\n",
    "\n",
    "Now we come to the data processing portion. To run data through a constructed TensorFlow graph, a session must be created to read input data into the graph and return output data. TensorFlow will only run a portion of the graph that is required to map a set of inputs (a dictionary of graph nodes, usually placeholders, as keys, and the input data as values) to an output graph node. This is invoked by the command:\n",
    "\n",
    "```\n",
    "tf.Session().run(output_node,\n",
    "                 {input_node_1: input_data_1, input_node_2: input_data_2, ...})\n",
    "```\n",
    "\n",
    "To test the helper function, we assign a jpeg string to the input placeholder, and return a 3D tensor result which is the normalized image.\n",
    "\n",
    "**Exercise:** Add more potentially useful assert statements to test the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the result of the function using a sample image client/cat_sample.jpg\n",
    "\n",
    "with open(\"client/cat_sample.jpg\", \"rb\") as imageFile:\n",
    "    jpeg_str = imageFile.read()\n",
    "    with tf.Session() as sess:\n",
    "        result = sess.run(test_decoded_tensor, feed_dict={test_jpeg_ph: jpeg_str})\n",
    "        assert result.shape == (224, 224, 3)\n",
    "        # TODO: Replace with assert statements to check max and min normalized pixel values\n",
    "        assert False\n",
    "        print('Hooray! JPEG decoding test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "The approach above uses vanilla TensorFlow to perform unit testing. You may notice that the code is more verbose than ideal, since you have to create a session, feed input through a dictionary, etc. We encourage the student to investigate some options below: \n",
    "\n",
    "[TensorFlow Eager](https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html) was introduced in TensorFlow 1.5 as a way to execute TensorFlow graphs in a way similar to numpy operations. After testing individual parts of the graph using Eager, you will need to rebuild a graph with the Eager option turned off in order to build a performance optimized TensorFlow graph. Also, keep in mind that you will need another virtual environment with TensorFlow 1.5 in order to run eager execution, which may not be compatible with TensorFlow Serving 1.4 used in this tutorial.\n",
    "\n",
    "[TensorFlow unit testing](https://www.tensorflow.org/api_guides/python/test) is a more software engineer oriented approach to run tests. By writing test classes that can be invoked individually when building the project, calling tf.test.main() will run all tests and return a list of ones that succeeded and failed, allowing you to inspect errors. Because we are in a notebook environment, such a test would not succeed due to an already running kernel that tf.test cannot access. The tests must be run from the command line, e.g. `python test_my_graph.py`.\n",
    "\n",
    "We've provided both eager execution and unit test examples in the [testing](./testing) directory showing how to unit test various components in this notebook. Note that because these examples contain the solution to exercises below, please complete all notebook exercises prior to reading through these examples.\n",
    "\n",
    "Now that we know how to run TensorFlow tests, let's create and test more helper functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Preprocessing Server Input\n",
    "\n",
    "**Exercise**: Messages from our [client](https://github.com/google-aai/tf-serving-k8s-tutorial/blob/master/client/resnet_client.py) arrive as a dictionary of the form {'images': array_of_jpeg_strings}. However, the ResNet network expects a 4D tensor, where dimension 0 corresponds to the index of an image, and the other dimensions correspond to pixels of each image. We will wrap our JPEG decoding helper function in another helper function that converts the client message into an array of 3D tensors, and then pack them into a 4D tensor. Follow the TODOs in the code below to complete the preprocess_input() helper function.\n",
    "\n",
    "**Note**: Serving input often differs significantly from training input! For instance, training data often comes in the form of a [TF Dataset](https://www.tensorflow.org/programmers_guide/datasets) with information such as labels, text, encoding, bounding boxes, etc. Our server-client architecture is very simple, since we simply want to send it JPEG images and receive classification results. \n",
    "\n",
    "**Useful References:**\n",
    "* [tf.map_fn](https://www.tensorflow.org/api_docs/python/tf/map_fn)\n",
    "* [tf.DType](https://www.tensorflow.org/api_docs/python/tf/DType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(features):\n",
    "    '''Function to preprocess client request before feeding into the network.\n",
    "    \n",
    "    Use tf.map_fn and the convert_jpeg_to_image() helper function to convert the\n",
    "    1D input tensor of jpeg strings into a list of single-precision floating\n",
    "    point 3D tensors, which are normalized pixel values for the images.\n",
    "    \n",
    "    Then stack and reshape this list of tensors into a 4D tensor with\n",
    "    appropriate dimensions.\n",
    "    \n",
    "    Args:\n",
    "      features: request received from our client,\n",
    "        a dictionary with a single element containing a tensor of multiple jpeg images\n",
    "        {'images' : 1D_tensor_of_jpeg_byte_strings}\n",
    "    \n",
    "    Returns:\n",
    "      a 4D tensor of normalized pixel values for the input images.\n",
    "      \n",
    "    '''\n",
    "    images = features['images']  # A tensor of tf.strings\n",
    "    processed_images = ???  # TODO: fill in the ???\n",
    "    processed_images = tf.stack(processed_images)  # Convert list of 3D tensors to a 4D tensor\n",
    "    processed_images = tf.reshape(tensor=processed_images,  # Reshaping informs Tensorflow of the final dimensions of the 4D tensor\n",
    "                                  shape=[-1, _DEFAULT_IMAGE_SIZE, _DEFAULT_IMAGE_SIZE, 3])\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Test the Preprocessing Helper Function\n",
    "\n",
    "**Exercise**: Recall that your client is sending a message of the format:\n",
    "\n",
    "```\n",
    "{'images': array_of_strings}\n",
    "```\n",
    "\n",
    "The array_of_strings can be arbitrary length, and requires an entrypoint through a [placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) that can read in an arbitrary length array of strings. Fix the shape parameter to allow for an arbitrary length string array as input.\n",
    "\n",
    "**Hint:** You need to define the `shape` parameter in tf.placeholder. `None` inside an array indicates that the length can vary along that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Test Input Preprocessing Network: only needs to be run once!\n",
    "test_jpeg_tensor = tf.placeholder(dtype=tf.string, shape=???, name='test_jpeg_tensor')  # A placeholder for a single string, which is a dimensionless (0D) tensor.\n",
    "test_processed_images = preprocess_input({'images': test_jpeg_tensor})  # Output node, which returns a 3D tensor after processing.\n",
    "\n",
    "# Print the graph elements to check shapes. ? indicates that Tensorflow does not know the length of those dimensions.\n",
    "print(test_jpeg_tensor)\n",
    "print(test_processed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test network using a sample image client/cat_sample.jpg\n",
    "\n",
    "with open(\"client/cat_sample.jpg\", \"rb\") as imageFile:\n",
    "    jpeg_str = imageFile.read()\n",
    "    with tf.Session() as sess:\n",
    "        result = sess.run(test_processed_images, feed_dict={test_jpeg_tensor: np.array([jpeg_str, jpeg_str])})  # Duplicate for length 2 array\n",
    "        assert result.shape == (2, 224, 224, 3)  # 4D tensor with first dimension length 2, since we have 2 images\n",
    "        # TODO: add a test for min and max normalized pixel values\n",
    "        assert False\n",
    "        # TODO: add a test to verify that the resulting tensor for image 0 and image 1 are identical.\n",
    "        assert False\n",
    "        print('Hooray! Input unit test succeeded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Postprocess Server Output\n",
    "\n",
    "**Exercise:** The ResNet50 model returns a Tensor of logits for each of its possible classes. The [client](./client/resnet_client.py), however, expects a response that consists of the top 5 likely classes for each image, and probabilities of each image belonging to those classes. Modify the output helper function to convert an array of logits to a dictionary that stores tensors of the top 5 classes and probabilities.\n",
    "\n",
    "**Useful References:**\n",
    "* [tf.nn.top_k](https://www.tensorflow.org/api_docs/python/tf/nn/top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 5\n",
    "\n",
    "def postprocess_output(logits, k=TOP_K):\n",
    "    '''Return top k classes and probabilities from class logits.'''\n",
    "    probs = tf.nn.softmax(logits)  # Converts logits to probabilities.\n",
    "    top_k_probs, top_k_classes = ???\n",
    "    return {'classes': top_k_classes, 'probabilities': top_k_probs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Test the Output Postprocessing Helper Function\n",
    "\n",
    "**Exercise:** Fill in the shape field for the logits placeholder\n",
    "\n",
    "**Hint:** how many image classes are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Test Output Postprocessing Network: only needs to be run once!\n",
    "test_logits_ph = tf.placeholder(dtype=tf.float32, shape=???, name='test_logits_placeholder')\n",
    "test_prediction_output = postprocess_output(test_logits_ph)\n",
    "\n",
    "# Print the graph elements to check shapes.\n",
    "print(test_logits_ph)\n",
    "print(test_prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test network\n",
    "with tf.Session() as sess:\n",
    "    logits = np.ones(1001)\n",
    "    result = sess.run(test_prediction_output, {test_logits_ph: logits})\n",
    "    classes = result['classes']\n",
    "    probs = result['probabilities']\n",
    "    # Inefficient but simple element-wise check\n",
    "    assert probs[1:].all() == probs[:-1].all()\n",
    "    # Check that they equal 1 / 1001\n",
    "    expected_probs = np.array(len(probs) * [1.0/1001.0])\n",
    "    assert probs.all() == expected_probs.all()\n",
    "    print('Hooray! Output unit test succeeded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Helper Function\n",
    "\n",
    "We will now integrate the input helper function, output helper function, and network together into a serving_input_to_output() function, which is called by the main model function (serving_model_fn()) above. This function defines an end-to-end graph thta takes an input jpeg tensor, converts it to a 4d floating point tensor, runs the tensor through the ResNet50 network, and postprocesses the output to return a dictionary of the top k predicted classes and probabilities.\n",
    "\n",
    "Normally, we would want to create an integration test for this end-to-end function. However, to avoid replicating the entire ResNet50 network and causing potential memory issues in a notebook environment, we instead provide an example of integration testing in the [Estimator Unit Test](./testing/estimator_unit_test.py) python file.\n",
    "\n",
    "**Exercise:** The function below works as is, but you may want to change the data_format argument below depending on whether you are deploying serving in a CPU only or GPU environment. For convolutional neural nets, it has been shown that placing your color channels ('channels_first') before your pixel dimensions in the image tensor significantly improves performance over 'channels_last'. \n",
    "\n",
    "**Note:** If you are running serving with GPU, feel free to try out both data formats, i.e. 'channels_first' and 'channels_last', to compare performances during serving. HOWEVER, in [the next notebook](./resnet_servable_validation.ipynb) where you will validate the servable model that you produced in this step, 'channels_last' is required due to limitations in the tf.contrib.predict package. If you want to validate your servable, we suggest you start by creating a servable with data format 'channels_last' for validation, then recreate a servable with 'channels_first' as this should also work without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_to_output(features, mode, k=TOP_K):\n",
    "    # Preprocess inputs before sending tensors to the network.\n",
    "    processed_images = preprocess_input(features)\n",
    "\n",
    "    # TODO: Feel free to use 'channels_first' or 'channels_last'\n",
    "    network = imagenet_resnet_v2(RESNET_SIZE, _LABEL_CLASSES, data_format='channels_last')\n",
    "\n",
    "    # NOTE: No need to change this, but is_training will always be false since we are predicting.\n",
    "    logits = network(\n",
    "      inputs=processed_images, is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "    # Postprocess outputs of network (logits) and send top k predictions back to client.\n",
    "    predictions = postprocess_output(logits, k=k)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Servable Model API Definition\n",
    "\n",
    "The last step in serving_model_fn() is to return an EstimatorSpec containing instructions for the Estimator to export a servable model. EstimatorSpec contains a field `export_outputs`, which defines the dictionary of fields that the servable model will return to a client upon receiving a request. To export the predictions dictionary above using Tf serving, you will need to assign the export_outputs parameter in EstimatorSpec.\n",
    "\n",
    "**Exercise:** Add a dictionary with a string key which will be the request.model_spec.signature_name that\n",
    "your client will call in [client/resnet_client.py](./client/resnet_client.py)\n",
    "Add a value that is tf.estimator.export.PredictOutput(outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_servable_estimator_spec(predictions, mode):\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=predictions,  # Note: This is not be used in serving, but must be provided for the Estimator API.\n",
    "      ??? # TODO: assign an appropriate dictionary to the export_outputs parameter here.\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Estimator\n",
    "\n",
    "Create an estimator with the serving_model_fn defined above. The estimator will load saved checkpoint data (model parameters from training) from the model_dir directory: namely, MODEL_DIR where we downloaded and extracted checkpoint files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=serving_model_fn,\n",
    "  model_dir=MODEL_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving input receiver function\n",
    "\n",
    "Finally, exporting the model requires a serving_input_receiver_fnn that explicitly tells the server what message format to expect from the client. \n",
    "\n",
    "**Exercise:** Replace the input to [build_raw_serving_input_receiver_fn](https://www.tensorflow.org/api_docs/python/tf/estimator/export/build_parsing_serving_input_receiver_fn) below with the expected format of features in the serving_model_fn, i.e. {'images': tf.placeholder(...)}. \n",
    "\n",
    "**Hint:** it's identical to the placeholder in your input unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "  return tf.estimator.export.build_raw_serving_input_receiver_fn(???)()  ## TODO: Add dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the servable model to disk\n",
    "\n",
    "Assuming all of your unit tests have succeeded, and your serving_model_fn() is implemented correctly, this step should successfully export a saved model to disk in the SERVING_DIR specified above. If not, look through the logs to find the point of failure in one of your above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.export_savedmodel(export_dir_base=SERVING_DIR,\n",
    "                            serving_input_receiver_fn=serving_input_receiver_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
